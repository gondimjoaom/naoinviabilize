{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.7.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.7.0/pt_core_news_sm-3.7.0-py3-none-any.whl (13.0 MB)\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from pt-core-news-sm==3.7.0) (3.7.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.12.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.66.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.7.4)\n",
      "Requirement already satisfied: jinja2 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (70.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2024.6.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from jinja2->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/caiopetruccirosa/anaconda3/envs/naoinviabilize/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-sm==3.7.0) (0.1.2)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# NOTE: This is ONLY necessary in jupyter notebook.\n",
    "# Details: Jupyter runs an event-loop behind the scenes.\n",
    "#          This results in nested event-loops when we start an event-loop to make async queries.\n",
    "#          This is normally not allowed, we use nest_asyncio to allow it for convenience.\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "from llama_index.core import Document, QueryBundle\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.tools import tool\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from groq import Groq, RateLimitError\n",
    "\n",
    "import spacy\n",
    "\n",
    "from glob import glob\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "from thefuzz import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().handlers = []\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_WINDOW_SIZE = 5\n",
    "SENTENCE_WINDOW_STRIDE = 2\n",
    "\n",
    "TOP_K = 4\n",
    "BM25_TOP_K = 1000\n",
    "\n",
    "RERANKER_MODEL = \"unicamp-dl/monoptt5-base\"\n",
    "\n",
    "LLM_MODEL = \"llama3-70b-8192\"\n",
    "LLM_CONTEXT_SIZE = 8192\n",
    "LLM_TEMPERATURE = 0\n",
    "LLM_TOP_P = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcription_documents(base_transcriptions_path):\n",
    "    transcription_documents = []\n",
    "\n",
    "    for transcriptions_path in glob(base_transcriptions_path):\n",
    "        with open(transcriptions_path) as f:\n",
    "            transcriptions = json.load(f)\n",
    "        \n",
    "        for transcription in transcriptions:\n",
    "            transcription_documents.append(\n",
    "                Document(\n",
    "                    text=transcription['transcription'],\n",
    "                    metadata={\n",
    "                        'title': transcription['title'],\n",
    "                        'publishing_date': transcription['publishing_date'],\n",
    "                        'quadro': transcription['quadro'],\n",
    "                        'hashtag': transcription['hashtag'],\n",
    "                    },\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    return transcription_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_split(documents, stride, window_size):\n",
    "    sentencizer = spacy.blank('pt')\n",
    "    sentencizer.add_pipe('sentencizer')\n",
    "    \n",
    "    window_documents = []\n",
    "\n",
    "    for document in documents:\n",
    "        text = document.text\n",
    "        paragraphs = text.split('\\n\\n') \n",
    "        paragraphs = [ p.replace('\\n', ' ').strip() for p in paragraphs if p.strip() ]\n",
    "        for paragraph in paragraphs:\n",
    "            p_sentencized = sentencizer(paragraph)\n",
    "            sentences = [sent.text.strip() for sent in p_sentencized.sents]\n",
    "            for i in range(0, len(sentences), stride):\n",
    "                window_text = ' '.join(sentences[i : min(len(sentences), i+window_size)]).strip()\n",
    "                window_metadata = document.metadata.copy()\n",
    "                window_metadata['parent_document_id'] = document.id_\n",
    "                window_documents.append(Document(text=window_text, metadata=window_metadata))\n",
    "\n",
    "    return window_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcriptions_nodes(transcription_documents):\n",
    "    transcription_window_documents = sliding_window_split(transcription_documents, SENTENCE_WINDOW_STRIDE, SENTENCE_WINDOW_SIZE)\n",
    "    \n",
    "    transcription_nodes = dict()\n",
    "    for document in transcription_window_documents:\n",
    "        new_node = TextNode(id=document.id_, text=document.text, metadata=document.metadata)\n",
    "        if new_node.metadata['title'] not in transcription_nodes.keys():\n",
    "            transcription_nodes[document.metadata['title']] = [new_node]\n",
    "        else:\n",
    "            transcription_nodes[document.metadata['title']].append(new_node)\n",
    "    \n",
    "    return transcription_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_documents = get_transcription_documents(\"../transcriptions-headless/*.json\")\n",
    "\n",
    "# transcription_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_nodes = get_transcriptions_nodes(transcription_documents)\n",
    "\n",
    "# transcription_nodes['milton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqClient:\n",
    "    '''\n",
    "    Interface for using the Groq API\n",
    "\n",
    "    Implements a rate limit control for multi-threading use.\n",
    "    '''\n",
    "\n",
    "    # documentacao dos parametros em: https://console.groq.com/docs/text-chat\n",
    "    _context_size = LLM_CONTEXT_SIZE\n",
    "    _temperature = LLM_TEMPERATURE\n",
    "    _top_p = LLM_TOP_P\n",
    "    _stop = None\n",
    "    _stream = False\n",
    "\n",
    "    # Mutex lock\n",
    "    _rate_lock = threading.Lock()\n",
    "\n",
    "    def __init__(self, llm_model, api_key):\n",
    "        '''\n",
    "        GroqClient constructor.\n",
    "        '''\n",
    "        self._client = Groq(api_key=api_key)\n",
    "        self._llm_model = llm_model\n",
    "\n",
    "\n",
    "    def __call__(self, prompt: str) -> str:\n",
    "        '''\n",
    "        Generates the model response\n",
    "\n",
    "        Args:\n",
    "            prompt (str): prompt to send to the model.\n",
    "\n",
    "        Returns:\n",
    "            str: model response.\n",
    "        '''\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            try:\n",
    "                GroqClient._rate_lock.acquire()\n",
    "                GroqClient._rate_lock.release()\n",
    "                chat_completion = self._client.chat.completions.create(\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": prompt,\n",
    "                        }\n",
    "                    ],\n",
    "                    model=self._llm_model,\n",
    "                    temperature=self._temperature,\n",
    "                    max_tokens=self._context_size,\n",
    "                    top_p=self._top_p,\n",
    "                    stop=self._stop,\n",
    "                    stream=self._stream,\n",
    "                )\n",
    "                done = True\n",
    "            except RateLimitError as exception:\n",
    "                GroqClient.error = exception\n",
    "                if not GroqClient._rate_lock.locked():\n",
    "                    GroqClient._rate_lock.acquire()\n",
    "                    time.sleep(1.75)\n",
    "                    GroqClient._rate_lock.release()\n",
    "\n",
    "        return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RAGResponse:\n",
    "    answer: str\n",
    "    contexts: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiIndexRetriever:\n",
    "    def __init__(self, indexes_nodes: Dict[str, List[TextNode]], top_k, bm25_top_k, reranker_model) -> None:\n",
    "        self.indexes: Dict[str, List[TextNode]] = indexes_nodes\n",
    "        \n",
    "        self.bm25_retrievers: Dict[str, BM25Retriever] = dict()\n",
    "        for index, nodes in indexes_nodes.items():\n",
    "            self.bm25_retrievers[index] = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=bm25_top_k)\n",
    "\n",
    "        self.reranker = SentenceTransformerRerank(top_n=top_k, model=reranker_model)\n",
    "        \n",
    "\n",
    "    def retrieve(self, index_name: str, query: str) -> List[str]:\n",
    "        if index_name not in self.indexes:\n",
    "            raise ValueError(f\"Index {index_name} not found\")\n",
    "        \n",
    "        retriever = self.bm25_retrievers[index_name]\n",
    "\n",
    "        retrieved_nodes = retriever.retrieve(query)\n",
    "        reranked_nodes = self.reranker.postprocess_nodes(\n",
    "            retrieved_nodes,\n",
    "            query_bundle=QueryBundle(query),\n",
    "        )\n",
    "\n",
    "        context_chunks = [ node.get_text() for node in reranked_nodes ]\n",
    "\n",
    "        return context_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexDetector:\n",
    "    def __init__(self, llm_model: str, index_names: List[str], GROQ_API_KEY: str = os.environ['GROQ_API_KEY']):\n",
    "        self._base_prompt = \\\n",
    "            \"Given the following Portuguese query, regarding an episode of a podcast, please tell me the title of the episode. \" \\\n",
    "            \"The query starts now: '{query}'.\" \\\n",
    "            \"You MUST answer with only the title of the episode.\"\n",
    "        \n",
    "        self.llm = GroqClient(llm_model=llm_model, api_key=GROQ_API_KEY)\n",
    "        self.llm_model_name = llm_model\n",
    "        self.index_names = index_names\n",
    "\n",
    "    def detect_index(self, query: str) -> str:\n",
    "        prompt = self._base_prompt.format(query=query)\n",
    "        raw_llm_guess = self.llm(prompt)\n",
    "        llm_guess = raw_llm_guess.strip().lower()\n",
    "\n",
    "        if llm_guess not in self.index_names:\n",
    "            index_name = llm_guess\n",
    "        else:\n",
    "            index_name, _ = process.extractOne(llm_guess, self.index_names)\n",
    "            \n",
    "        return index_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGGenerator:\n",
    "    def __init__(self, llm_model, GROQ_API_KEY=os.environ['GROQ_API_KEY']):\n",
    "        self._base_prompt = \\\n",
    "            \"Consider the following context passages of a podcast episode and answer the given question.\" \\\n",
    "            \"You MUST answer the question only in Portuguese.\" \\\n",
    "            \"\\n\\n\" \\\n",
    "            \"{context_passages}\" \\\n",
    "            \"\\n\\n\" \\\n",
    "            \"If there is not enough information in the context passages, answer \\\"Não há informação suficiente no episódio.\\\".\" \\\n",
    "            \"\\n\\n\" \\\n",
    "            \"Question: {query}\"\n",
    "        \n",
    "        self.llm = GroqClient(llm_model=llm_model, api_key=GROQ_API_KEY)\n",
    "        self.llm_model_name = llm_model\n",
    "\n",
    "    def generate_answer(self, query: str, contexts: List[str]) -> str:\n",
    "        context_passages = \"\\n\\n\".join([ f\"Context {i}: {context}\" for i, context in enumerate(contexts, 1) ])\n",
    "        prompt = self._base_prompt.format(query=query, context_passages=context_passages)\n",
    "        answer = self.llm(prompt)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        retriever: MultiIndexRetriever, \n",
    "        index_detector: IndexDetector,\n",
    "        generator: RAGGenerator,\n",
    "    ) -> None:\n",
    "        self.index_detector: IndexDetector = index_detector\n",
    "        self.retriever: MultiIndexRetriever = retriever\n",
    "        self.generator: RAGGenerator = generator\n",
    "\n",
    "    def __call__(self, query: str) -> RAGResponse:\n",
    "        index_name = self.index_detector.detect_index(query)\n",
    "        context_chunks = self.retriever.retrieve(index_name, query)\n",
    "        answer = self.generator.generate_answer(query, context_chunks)\n",
    "        return RAGResponse(answer, context_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "API_KEY_1 = getpass() #pers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'mário'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_detector = IndexDetector(llm_model=LLM_MODEL, index_names=list(transcription_nodes.keys()), GROQ_API_KEY=API_KEY_1)\n",
    "index_name = index_detector.detect_index('No episódio \\'mário\\', quem foi diagnosticado com câncer de bexiga na história?')\n",
    "index_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A Bia é a Bia Suzuki, uma mulher de 30 anos que mora em São José do Rio Preto, interior de São Paulo, e que recebeu o diagnóstico de câncer de intestino aos 25 anos.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_contexts = [\n",
    "    \"Eu sou Bia Suzuki, tenho 30 anos, moro em São José do Rio Preto, interior de São Paulo... eu recebi o diagnóstico de câncer de intestino aos 25 anos...\",\n",
    "    \"O câncer colorretal é a terceira neoplasia mais frequente e a segunda de maior mortalidade no mundo...\",\n",
    "    \"Eu achei que não é impossível viver bem com bolsinha e eu sou uma prova disso.\"\n",
    "]\n",
    "example_question = \"No episódio 'bia', quem é a Bia e o que aconteceu com ela?\"\n",
    "\n",
    "rag_generator = RAGGenerator(llm_model=LLM_MODEL)\n",
    "rag_generator.generate_answer(example_question, example_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "API_KEY_2 = getpass() #UNICAMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.3.1+cu121 available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmg/Documents/Unicamp/2024.1/ia024/trab-final/env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at unicamp-dl/monoptt5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "multi_index_retriever = MultiIndexRetriever(\n",
    "    indexes_nodes=transcription_nodes,\n",
    "    top_k=TOP_K,\n",
    "    bm25_top_k=BM25_TOP_K,\n",
    "    reranker_model=RERANKER_MODEL\n",
    ")\n",
    "index_detector = IndexDetector(llm_model=LLM_MODEL, index_names=list(transcription_nodes.keys()), GROQ_API_KEY=API_KEY_1)\n",
    "rag_generator = RAGGenerator(llm_model=LLM_MODEL, GROQ_API_KEY=API_KEY_2)\n",
    "\n",
    "rag_pipeline = RAGPipeline(\n",
    "    retriever=multi_index_retriever,\n",
    "    index_detector=index_detector,\n",
    "    generator=rag_generator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliando RAG apenas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from datasets import Dataset \n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_relevancy, answer_similarity, answer_correctness\n",
    "from ragas import evaluate\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "API_KEY_3 = getpass() #mob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use pytorch device_name: cuda\n",
      "Load pretrained SentenceTransformer: sentence-transformers/multi-qa-mpnet-base-cos-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jmg/Documents/Unicamp/2024.1/ia024/trab-final/env/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# llm = GroqClient(api_key=API_KEY, llm_model=\"llama3-70b-8192\")\n",
    "langchain_embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/multi-qa-mpnet-base-cos-v1\")\n",
    "langchain_groq_completion = ChatGroq(\n",
    "    temperature=0,\n",
    "    model_name=\"llama3-70b-8192\",\n",
    "    api_key=API_KEY_3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ragas(llm_model, embeddings, questions=None, answers=None, gts=None, contexts=None):\n",
    "    data = {\n",
    "            'question': questions,\n",
    "            'answer': gts if answers is None else answers,\n",
    "            'ground_truth': gts,\n",
    "            'contexts': contexts\n",
    "        }\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    score = evaluate(\n",
    "                    dataset,\n",
    "                    # metrics=[answer_relevancy, faithfulness, context_relevancy, answer_similarity, answer_correctness],\n",
    "                    # metrics=[answer_relevancy, faithfulness, context_relevancy, answer_correctness],\n",
    "                    metrics=[answer_relevancy, faithfulness, context_relevancy],\n",
    "                    # metrics=[answer_relevancy],\n",
    "                    llm=llm_model,\n",
    "                    embeddings=embeddings)\n",
    "    score = score.to_pandas()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5a368795134cf8a96a5d3d85f6bdfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908185b516334bb69a3691f27af0ec8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9df024abb0746ac913149350dc20f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7c6de00de542e8bfe62a269ea3b8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1e84347dc0b4d54b0baba697e53b0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying request to /openai/v1/chat/completions in 0.872442 seconds\n",
      "Retrying request to /openai/v1/chat/completions in 0.976006 seconds\n",
      "Retrying request to /openai/v1/chat/completions in 0.875989 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 23.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 29.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 24.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 29.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 28.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 6.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 7.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 9.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 8.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 8.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 10.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 27.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 26.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c3c20be5d04367906d8982974fe57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23807e8c85d447baa919b2c88bc5d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying request to /openai/v1/chat/completions in 0.880380 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 8.000000 seconds\n",
      "Retrying request to /openai/v1/chat/completions in 8.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 3.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 8.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 1.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 14.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 7.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 7.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 6.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 6.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 7.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 7.000000 seconds\n",
      "Retrying request to /openai/v1/chat/completions in 7.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 9.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\n",
      "Retrying request to /openai/v1/chat/completions in 40.000000 seconds\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6724ca3e60294141a72ed6d77b621117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8e273c7903424eb24ca9b92e51f9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for quadro in glob(\"../perguntas/*perguntas.json\"):\n",
    "    df_quadro = pd.DataFrame()\n",
    "    with open(quadro, \"r\") as jsonFile:\n",
    "        eps = json.load(jsonFile)\n",
    "    c = 0\n",
    "    for ep in tqdm(eps[::-1]):\n",
    "        # perguntas = []\n",
    "        # respostas = []\n",
    "        # contextos = []\n",
    "        # gts = []\n",
    "        for pergunta, gt in zip(ep[\"Perguntas\"], ep[\"Respostas\"]):\n",
    "            perguntas = []\n",
    "            respostas = []\n",
    "            contextos = []\n",
    "            gts = []\n",
    "            done_rag = False\n",
    "            while not done_rag:\n",
    "                try:\n",
    "                    rag_answer = rag_pipeline(pergunta)\n",
    "                except:\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                done_rag = True\n",
    "            perguntas.append(pergunta)\n",
    "            respostas.append(rag_answer.answer)\n",
    "            contextos.append(rag_answer.contexts)\n",
    "            gts.append(gt)\n",
    "            done_ragas = False\n",
    "            while not done_ragas:\n",
    "                try:\n",
    "                    df_score = evaluate_ragas(langchain_groq_completion, langchain_embeddings_model, perguntas, respostas, gts, contextos)\n",
    "                except:\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "                done_ragas = True\n",
    "            df_quadro = pd.concat([df_quadro, df_score])\n",
    "    df_quadro[\"quadro\"] = ep[\"quadro\"]\n",
    "    df = pd.concat([df, df_quadro])\n",
    "        # c -=- 1\n",
    "        # if c == 2: break\n",
    "            # break\n",
    "        # break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'alarme'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep[\"quadro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>context_relevancy</th>\n",
       "      <th>quadro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No episódio 'alexandra', quem é a protagonista...</td>\n",
       "      <td>A protagonista da história contada é Alexandra.</td>\n",
       "      <td>Alexandra Mendes Leite</td>\n",
       "      <td>[E nos artesanatos, foi a reciclagem. Trabalha...</td>\n",
       "      <td>0.374252</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>alarme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No episódio 'alexandra', o que happeniu com a ...</td>\n",
       "      <td>Não há informação suficiente no episódio.</td>\n",
       "      <td>Um grande incêndio que destruiu grande parte d...</td>\n",
       "      <td>[Então era péssimo. — Com 12 anos a Alexandra ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.190476</td>\n",
       "      <td>alarme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No episódio 'alexandra', como a comunidade Chi...</td>\n",
       "      <td>Não há informação suficiente no episódio.</td>\n",
       "      <td>Através do apoio do Fundo Brasil e da capacita...</td>\n",
       "      <td>[Vocês lembram que eu falei que a Alexandra te...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>alarme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No episódio 'alessandra', quem é Alessandra Fé...</td>\n",
       "      <td>Alessandra é a mãe que compartilhou sua experi...</td>\n",
       "      <td>Alessandra Félix é uma pedagoga que criou o co...</td>\n",
       "      <td>[Eu não queria aquilo. Mãe nenhuma tem manual ...</td>\n",
       "      <td>0.465796</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>alarme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No episódio 'alessandra', o que é o coletivo V...</td>\n",
       "      <td>Não há informação suficiente no episódio.</td>\n",
       "      <td>O coletivo Vozes de Mães e Familiares do Siste...</td>\n",
       "      <td>[Aline Andrade: Meu nome é Aline Andrade e eu ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>alarme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No episódio 'alessandra', por que o Fundo Bras...</td>\n",
       "      <td>Não há informação suficiente no episódio.</td>\n",
       "      <td>O Fundo Brasil de Direitos Humanos apoia proje...</td>\n",
       "      <td>[A família toda da Alessandra é de uma cidade ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>alarme</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  No episódio 'alexandra', quem é a protagonista...   \n",
       "1  No episódio 'alexandra', o que happeniu com a ...   \n",
       "2  No episódio 'alexandra', como a comunidade Chi...   \n",
       "0  No episódio 'alessandra', quem é Alessandra Fé...   \n",
       "1  No episódio 'alessandra', o que é o coletivo V...   \n",
       "2  No episódio 'alessandra', por que o Fundo Bras...   \n",
       "\n",
       "                                              answer  \\\n",
       "0    A protagonista da história contada é Alexandra.   \n",
       "1          Não há informação suficiente no episódio.   \n",
       "2          Não há informação suficiente no episódio.   \n",
       "0  Alessandra é a mãe que compartilhou sua experi...   \n",
       "1          Não há informação suficiente no episódio.   \n",
       "2          Não há informação suficiente no episódio.   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0                             Alexandra Mendes Leite   \n",
       "1  Um grande incêndio que destruiu grande parte d...   \n",
       "2  Através do apoio do Fundo Brasil e da capacita...   \n",
       "0  Alessandra Félix é uma pedagoga que criou o co...   \n",
       "1  O coletivo Vozes de Mães e Familiares do Siste...   \n",
       "2  O Fundo Brasil de Direitos Humanos apoia proje...   \n",
       "\n",
       "                                            contexts  answer_relevancy  \\\n",
       "0  [E nos artesanatos, foi a reciclagem. Trabalha...          0.374252   \n",
       "1  [Então era péssimo. — Com 12 anos a Alexandra ...          0.000000   \n",
       "2  [Vocês lembram que eu falei que a Alexandra te...          0.000000   \n",
       "0  [Eu não queria aquilo. Mãe nenhuma tem manual ...          0.465796   \n",
       "1  [Aline Andrade: Meu nome é Aline Andrade e eu ...          0.000000   \n",
       "2  [A família toda da Alessandra é de uma cidade ...          0.000000   \n",
       "\n",
       "   faithfulness  context_relevancy  quadro  \n",
       "0          1.00           0.200000  alarme  \n",
       "1          0.00           0.190476  alarme  \n",
       "2          0.00           0.047619  alarme  \n",
       "0          0.25           0.235294  alarme  \n",
       "1          0.00           0.352941  alarme  \n",
       "2          0.00           0.250000  alarme  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"quadro\"] = ep[\"quadro\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt adaptado a partir da combinação dos seguintes prompts:\n",
    "#   - https://github.com/run-llama/llama_index/blob/a87b63fce3cc3d24dc71ae170a8d431440025565/llama_index/agent/react/prompts.py\n",
    "#   - https://smith.langchain.com/hub/hwchase17/react-chat\n",
    "\n",
    "REACT_CHAT_SYSTEM_HEADER = \"\"\"\\\n",
    "You are designed to help answering questions regarding the content of the Não Inviabilize podcast episodes.\n",
    "You are a Large Language Model trained by Meta AI. As a language model, you are able to generate human-like text based on the input you receive, allowing yourself to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
    "You are constantly learning and improving, and your capabilities are constantly evolving. You are able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions.\n",
    "Additionally, you are able to generate your own text based on the input it receives, allowing yourself to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
    "Overall, you are a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. \n",
    "When the user needs help with a specific question about something that was discussed on an specific episode of the Não Inviabilize podcast, you are here to assist.\n",
    "\n",
    "TOOLS:\n",
    "------\n",
    "You have access to a variety of tools that can help you get information to answer questions.\n",
    "You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand.\n",
    "This may require breaking the task into subtasks and using different tools to complete each subtask.\n",
    "You have access to the following tools:\n",
    "{tools}\n",
    "\n",
    "To use a tool, please use the following format:\n",
    "```\n",
    "Thought: Do I need to use a tool? Yes\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "```\n",
    "\n",
    "When you have a response to say to the Human, or if you do not need to use a tool, you MUST use the format:\n",
    "```\n",
    "Thought: Do I need to use a tool? No\n",
    "Final Answer: [your response here]\n",
    "```\n",
    "But be careful, you MUST answer the question in Portuguese only!\n",
    "\n",
    "If there is not enough information in the context passages, answer \"Não há informação suficiente no episódio.\"\n",
    "\n",
    "Begin!\n",
    "User question: {input}\n",
    "\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "class ReActRAGPipeline:\n",
    "    def __init__(\n",
    "        self,\n",
    "        retriever: MultiIndexRetriever, \n",
    "        index_detector: IndexDetector,\n",
    "        verbose: bool = False,\n",
    "    ) -> None:\n",
    "        self.index_detector: IndexDetector = index_detector\n",
    "        self.retriever: MultiIndexRetriever = retriever\n",
    "\n",
    "        base_prompt = PromptTemplate.from_template(REACT_CHAT_SYSTEM_HEADER)\n",
    "        llm = ChatGroq(\n",
    "            temperature=LLM_TEMPERATURE,\n",
    "            model_name=LLM_MODEL,\n",
    "            api_key=os.environ['GROQ_API_KEY']\n",
    "        )\n",
    "\n",
    "        tools = [self._search_episode_title, self._search_relevant_passages]\n",
    "        agent = create_react_agent(llm, tools, base_prompt)\n",
    "        agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=verbose, handle_parsing_errors=True)\n",
    "\n",
    "        self._search_history = []\n",
    "        self._search_history_lock = threading.Lock()\n",
    "\n",
    "        self.agent_executor = agent_executor\n",
    "\n",
    "    @tool\n",
    "    def _search_relevant_passages(self, episode_title: str, question: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Returns a sequence of relevant contexts passages for the given question about a specific episode in the podcast.\n",
    "        In order to perform this search, you must provide the question in Portuguese and the title of the episode.\n",
    "        \"\"\"\n",
    "        contexts = self.retriever.retrieve(episode_title, question)\n",
    "        self._search_history += contexts\n",
    "        return contexts\n",
    "\n",
    "\n",
    "    @tool\n",
    "    def _search_episode_title(self, question: str) -> list[str]:\n",
    "        \"\"\"\n",
    "        Returns the title of the episode that is being asked about in the given question.\n",
    "        In order to perform this search, you must provide the question in Portuguese.\n",
    "        \"\"\"\n",
    "        return self.index_detector.detect_index(question)\n",
    "    \n",
    "    def generate_answer(self, question: str) -> RAGResponse:\n",
    "        self._search_history_lock.acquire()\n",
    "        self._search_history = []\n",
    "\n",
    "        output = self.agent_executor.invoke({\"input\": \"No episódio 'bia', quem é a Bia e o que aconteceu com ela?\"})\n",
    "        answer = output['output']\n",
    "        contexts = self._search_history[:]\n",
    "        \n",
    "        self._search_history = []\n",
    "        self._search_history_lock.release()\n",
    "\n",
    "        return RAGResponse(answer, contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "react_rag_pipeline = ReActRAGPipeline(\n",
    "    retriever=multi_index_retriever, \n",
    "    index_detector=index_detector,\n",
    ")\n",
    "react_rag_pipeline.generate_answer(\"No episódio 'bia', quem é a Bia e o que aconteceu com ela?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroqClient:\n",
    "    '''\n",
    "    Interface for using the Groq API\n",
    "\n",
    "    Implements a rate limit control for multi-threading use.\n",
    "    '''\n",
    "\n",
    "    # documentacao dos parametros em: https://console.groq.com/docs/text-chat\n",
    "    _model_name = LLM_MODEL_NAME\n",
    "    _context_size = LLM_CONTEXT_SIZE\n",
    "    _temperature = LLM_TEMPERATURE\n",
    "    _top_p = LLM_TOP_P\n",
    "    _stop = None\n",
    "    _stream = False\n",
    "\n",
    "    # Mutex lock\n",
    "    _rate_lock = threading.Lock()\n",
    "\n",
    "    def __init__(self, api_key):\n",
    "        '''\n",
    "        GroqClient constructor.\n",
    "        '''\n",
    "        self._client = Groq(api_key=api_key)\n",
    "\n",
    "\n",
    "    def __call__(self, prompt: str) -> str:\n",
    "        '''\n",
    "        Generates the model response\n",
    "\n",
    "        Args:\n",
    "            prompt (str): prompt to send to the model.\n",
    "\n",
    "        Returns:\n",
    "            str: model response.\n",
    "        '''\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            try:\n",
    "                GroqClient._rate_lock.acquire()\n",
    "                GroqClient._rate_lock.release()\n",
    "                chat_completion = self._client.chat.completions.create(\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"system\",\n",
    "                            \"content\": prompt,\n",
    "                        }\n",
    "                    ],\n",
    "                    model=self._model_name,\n",
    "                    temperature=self._temperature,\n",
    "                    max_tokens=self._context_size,\n",
    "                    top_p=self._top_p,\n",
    "                    stop=self._stop,\n",
    "                    stream=self._stream,\n",
    "                )\n",
    "                done = True\n",
    "            except RateLimitError as exception:\n",
    "                GroqClient.error = exception\n",
    "                if not GroqClient._rate_lock.locked():\n",
    "                    GroqClient._rate_lock.acquire()\n",
    "                    time.sleep(1.75)\n",
    "                    GroqClient._rate_lock.release()\n",
    "\n",
    "        return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatCroqWrapper(ChatGroq):\n",
    "    _request_lock = None\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if ChatCroqWrapper._request_lock is None:\n",
    "            ChatCroqWrapper._request_lock = threading.Lock()\n",
    "\n",
    "        return super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _generate(self, *args, **kwargs):\n",
    "        result = None\n",
    "        while result is not None:\n",
    "            try:\n",
    "                GroqClient._rate_lock.acquire()\n",
    "                GroqClient._rate_lock.release()\n",
    "                result = super()._generate(*args, **kwargs)\n",
    "            except RateLimitError as exception:\n",
    "                GroqClient.error = exception\n",
    "                if not GroqClient._rate_lock.locked():\n",
    "                    GroqClient._rate_lock.acquire()\n",
    "                    time.sleep(2)\n",
    "                    GroqClient._rate_lock.release()\n",
    "        return result\n",
    "\n",
    "    async def _agenerate(self, *args, **kwargs):\n",
    "        result = None\n",
    "        while result is not None:\n",
    "            try:\n",
    "                GroqClient._rate_lock.acquire()\n",
    "                GroqClient._rate_lock.release()\n",
    "                result = super()._agenerate(*args, **kwargs)\n",
    "            except RateLimitError as exception:\n",
    "                GroqClient.error = exception\n",
    "                if not GroqClient._rate_lock.locked():\n",
    "                    GroqClient._rate_lock.acquire()\n",
    "                    time.sleep(2)\n",
    "                    GroqClient._rate_lock.release()\n",
    "        return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
